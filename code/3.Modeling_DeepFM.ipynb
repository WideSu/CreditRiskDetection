{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ayFCnk3_hDyL"
      },
      "outputs": [],
      "source": [
        "from multiprocessing.sharedctypes import Value\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DeepFM(nn.Module):\n",
        "    def __init__(self, args) -> None:\n",
        "        super(DeepFM, self).__init__()\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = args['gpuid']\n",
        "\n",
        "        self.lr = args['lr']\n",
        "        self.l2_reg = args['l2_reg']\n",
        "        self.epochs = args['epochs']\n",
        "\n",
        "        self.num_fetures = args['num_features']\n",
        "        self.emb_dim = args['embedding_dim']\n",
        "        self.feature_embs = nn.Embedding(sum(args['field_size']), args['embedding_dim'])\n",
        "        self.bias_embs = nn.Embedding(sum(args['field_size']), 1)\n",
        "\n",
        "        # self.num_layers = args['num_layers'] # 2\n",
        "        self.deep_neurons = args['dense_size']\n",
        "        self.early_stop = True\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.batch_norm = args['batch_norm']\n",
        "        self.opt = args['opt_name']\n",
        "\n",
        "        if args['deep_layer_act'] == 'relu':\n",
        "            self.deep_layer_act = nn.ReLU()\n",
        "        else:\n",
        "            raise ValueError('Invalid activation function name for deep layers')\n",
        "\n",
        "        self.dropout_fm_1o = nn.Dropout(p=args['1o_dropout_p'])\n",
        "        self.dropout_fm_2o = nn.Dropout(p=args['2o_dropout_p'])\n",
        "\n",
        "        deep_modules = []\n",
        "        layers_size = [self.num_fetures * self.emb_dim] + args['dense_size']\n",
        "        for in_size, out_size in zip(layers_size[:-1], layers_size[1:]):\n",
        "            deep_modules.append(nn.Linear(in_size, out_size))\n",
        "            if self.batch_norm:\n",
        "                deep_modules.append(nn.BatchNorm1d(num_features=out_size))\n",
        "            deep_modules.append(self.deep_layer_act)\n",
        "            deep_modules.append(nn.Dropout(p=args['deep_dropout_p']))\n",
        "        self.deep = nn.Sequential(*deep_modules)\n",
        "\n",
        "        self.output = nn.Linear(args['dense_size'][-1] + self.num_fetures + self.emb_dim, 1, bias=False) # concat projection\n",
        "\n",
        "        # self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.feature_embs.weight, std=0.01)\n",
        "\n",
        "        glorot = np.sqrt(2.0 / (self.num_fetures * self.emb_dim + self.deep_neurons[0]))\n",
        "\n",
        "        for la in self.deep:\n",
        "            if isinstance(la, nn.Linear):\n",
        "                nn.init.normal_(la.weight, std=glorot)\n",
        "                nn.init.constant_(la.bias, 0.)\n",
        "                glorot = np.sqrt(2.0 / (la.weight.size()[0] + la.weight.size()[1]))\n",
        "\n",
        "        glorot = np.sqrt(2.0 / (self.deep_neurons[-1] + self.num_fetures + self.emb_dim + 1))\n",
        "        nn.init.normal_(self.output.weight, std=glorot)\n",
        "\n",
        "    def forward(self, idxs, vals): # idx/vals: batchsize * feature_size\n",
        "        feat_emb = self.feature_embs(idxs)  # batch_size * feature_size * embedding_size\n",
        "        feat_emb = torch.multiply(feat_emb, torch.reshape(vals, [feat_emb.size(0), feat_emb.size(1), 1]).expand([feat_emb.size(0), feat_emb.size(1), self.emb_dim])) # batch_size * feature_size * embedding_size\n",
        "        # first order part\n",
        "        y_first_order = self.bias_embs(idxs) # batch_size * feature_size * 1\n",
        "        y_first_order = torch.multiply(y_first_order, torch.reshape(vals, [feat_emb.size(0), feat_emb.size(1), 1])) # batch_size * feature_size * 1\n",
        "        y_first_order = torch.sum(y_first_order, dim=2) # batch_size * feature_size\n",
        "        y_first_order = self.dropout_fm_1o(y_first_order) # batch_size * feature_size\n",
        "        # second order part\n",
        "        summed_features_emb = torch.sum(feat_emb, 1)  # batch_size * embedding_size\n",
        "        summed_features_emb_square = torch.square(summed_features_emb) # batch_size * embedding_size\n",
        "        squared_features_emb = torch.square(feat_emb)  # batch_size * feature_size * embedding_size\n",
        "        squared_sum_features_emb = torch.sum(squared_features_emb, 1)  # batch_size * embedding_size\n",
        "        y_second_order = 0.5 * torch.subtract(summed_features_emb_square, squared_sum_features_emb) # batch_size * embedding_size\n",
        "        y_second_order = self.dropout_fm_2o(y_second_order) # batch_size * embedding_size\n",
        "        # deep part\n",
        "        y_deep = feat_emb.view(feat_emb.size(0), -1) # batch_size * (feature_size * embedding_size)\n",
        "        y_deep = self.deep(y_deep)\n",
        "        \n",
        "        concat_input = torch.cat([y_first_order, y_second_order, y_deep], dim=1) # batchsize * (embedding_size+feature_size+last_layer_out_size)\n",
        "        output = self.output(concat_input)   # batch_size * 1\n",
        "        out = torch.sigmoid(output)\n",
        "\n",
        "        return out.view(-1)\n",
        "\n",
        "    def fit(self, train_loader, valid_loader=None):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        if self.opt == 'adam':\n",
        "            optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "        elif self.opt == 'adagrad':\n",
        "            optimizer = optim.Adagrad(self.parameters(), lr=self.lr)\n",
        "        elif self.opt == 'sgd':\n",
        "            optimizer = optim.SGD(self.parameters(), lr=self.lr)\n",
        "        else:\n",
        "            raise ValueError(f'Invalid optimizer name: {self.opt}')\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss(reduction='sum') # CE_log_loss for binary classification\n",
        "        \n",
        "        last_loss = 0.\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            self.train()\n",
        "            current_loss = 0.\n",
        "            total_sample_num = 0\n",
        "            for labels, idxs, vals in train_loader:\n",
        "                total_sample_num += labels.size()[0]\n",
        "                if torch.cuda.is_available():\n",
        "                    labels = labels.cuda()\n",
        "                    idxs = idxs.cuda()\n",
        "                    vals = vals.cuda()\n",
        "                else:\n",
        "                    labels = labels.cpu()\n",
        "                    idxs = idxs.cpu()\n",
        "                    vals = vals.cpu()\n",
        "                self.zero_grad()\n",
        "                # TODO remember to let batches in loader put on GPU or CPU\n",
        "                prediction = self.forward(idxs, vals)\n",
        "                loss = criterion(prediction, labels)\n",
        "                loss += self.l2_reg * self.output.weight.norm()\n",
        "                for la in self.deep:\n",
        "                    if la is nn.Linear:\n",
        "                        loss += self.l2_reg * la.weight.norm()\n",
        "                if torch.isnan(loss):\n",
        "                    raise ValueError(f'Loss=Nan or Infinity: current settings does not fit the recommender')\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                current_loss += loss.item()\n",
        "\n",
        "            print(f'[Epoch {epoch:03d}] - training loss={current_loss / total_sample_num:.4f}')\n",
        "            delta_loss = float(current_loss - last_loss)\n",
        "            if (abs(delta_loss) < 1e-5) and self.early_stop:\n",
        "                print('Satisfy early stop mechanism')\n",
        "                break\n",
        "            else:\n",
        "                last_loss = current_loss\n",
        "\n",
        "            if valid_loader is not None:\n",
        "                self.eval()\n",
        "                # TODO if need valdiation\n",
        "                pass\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        self.eval()\n",
        "        _, idxs, vals = next(iter(test_loader))\n",
        "        idxs = idxs.to(self.device)\n",
        "        vals = vals.to(self.device)\n",
        "        preds = self.forward(idxs, vals).cpu().detach()\n",
        "\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score, recall_score\n",
        "\n",
        "train_data = pd.read_csv('./train_data.csv').reset_index(drop=True)\n",
        "test_data = pd.read_csv('./test_data.csv').reset_index(drop=True)\n",
        "\n",
        "category_cols = ['FLAG_DOCUMENT_14_0.0',\n",
        " 'FLAG_DOCUMENT_14_1.0',\n",
        " 'FLAG_DOCUMENT_2_0.0',\n",
        " 'FLAG_PHONE_0.0',\n",
        " 'FLAG_PHONE_1.0',\n",
        " 'FLAG_DOCUMENT_9_0.0',\n",
        " 'FLAG_DOCUMENT_9_1.0',\n",
        " 'FLAG_DOCUMENT_11_0.0',\n",
        " 'FLAG_DOCUMENT_11_1.0',\n",
        " 'FLAG_DOCUMENT_20_0.0',\n",
        " 'FLAG_DOCUMENT_20_1.0',\n",
        " 'FLAG_DOCUMENT_19_0.0',\n",
        " 'FLAG_DOCUMENT_19_1.0',\n",
        " 'WEEKDAY_APPR_PROCESS_START_FRIDAY',\n",
        " 'WEEKDAY_APPR_PROCESS_START_MONDAY',\n",
        " 'WEEKDAY_APPR_PROCESS_START_SATURDAY',\n",
        " 'WEEKDAY_APPR_PROCESS_START_SUNDAY',\n",
        " 'WEEKDAY_APPR_PROCESS_START_THURSDAY',\n",
        " 'WEEKDAY_APPR_PROCESS_START_TUESDAY',\n",
        " 'WEEKDAY_APPR_PROCESS_START_WEDNESDAY',\n",
        " 'FLAG_DOCUMENT_16_0.0',\n",
        " 'FLAG_DOCUMENT_16_1.0',\n",
        " 'FLAG_DOCUMENT_12_0.0',\n",
        " 'FLAG_MOBIL_1.0',\n",
        " 'FLAG_DOCUMENT_7_0.0',\n",
        " 'FLAG_DOCUMENT_10_0.0',\n",
        " 'NAME_INCOME_TYPE_Commercial associate',\n",
        " 'NAME_INCOME_TYPE_Pensioner',\n",
        " 'NAME_INCOME_TYPE_State servant',\n",
        " 'NAME_INCOME_TYPE_Working',\n",
        " 'NAME_CONTRACT_TYPE_Cash loans',\n",
        " 'NAME_CONTRACT_TYPE_Revolving loans',\n",
        " 'FLAG_DOCUMENT_17_0.0',\n",
        " 'FLAG_DOCUMENT_17_1.0',\n",
        " 'NAME_HOUSING_TYPE_Co-op apartment',\n",
        " 'NAME_HOUSING_TYPE_House / apartment',\n",
        " 'NAME_HOUSING_TYPE_Municipal apartment',\n",
        " 'NAME_HOUSING_TYPE_Office apartment',\n",
        " 'NAME_HOUSING_TYPE_Rented apartment',\n",
        " 'NAME_HOUSING_TYPE_With parents',\n",
        " 'ORGANIZATION_TYPE_Advertising',\n",
        " 'ORGANIZATION_TYPE_Agriculture',\n",
        " 'ORGANIZATION_TYPE_Bank',\n",
        " 'ORGANIZATION_TYPE_Business Entity Type 1',\n",
        " 'ORGANIZATION_TYPE_Business Entity Type 2',\n",
        " 'ORGANIZATION_TYPE_Business Entity Type 3',\n",
        " 'ORGANIZATION_TYPE_Cleaning',\n",
        " 'ORGANIZATION_TYPE_Construction',\n",
        " 'ORGANIZATION_TYPE_Culture',\n",
        " 'ORGANIZATION_TYPE_Electricity',\n",
        " 'ORGANIZATION_TYPE_Emergency',\n",
        " 'ORGANIZATION_TYPE_Government',\n",
        " 'ORGANIZATION_TYPE_Hotel',\n",
        " 'ORGANIZATION_TYPE_Housing',\n",
        " 'ORGANIZATION_TYPE_Industry: type 1',\n",
        " 'ORGANIZATION_TYPE_Industry: type 10',\n",
        " 'ORGANIZATION_TYPE_Industry: type 11',\n",
        " 'ORGANIZATION_TYPE_Industry: type 12',\n",
        " 'ORGANIZATION_TYPE_Industry: type 13',\n",
        " 'ORGANIZATION_TYPE_Industry: type 2',\n",
        " 'ORGANIZATION_TYPE_Industry: type 3',\n",
        " 'ORGANIZATION_TYPE_Industry: type 4',\n",
        " 'ORGANIZATION_TYPE_Industry: type 5',\n",
        " 'ORGANIZATION_TYPE_Industry: type 7',\n",
        " 'ORGANIZATION_TYPE_Industry: type 8',\n",
        " 'ORGANIZATION_TYPE_Industry: type 9',\n",
        " 'ORGANIZATION_TYPE_Insurance',\n",
        " 'ORGANIZATION_TYPE_Kindergarten',\n",
        " 'ORGANIZATION_TYPE_Legal Services',\n",
        " 'ORGANIZATION_TYPE_Medicine',\n",
        " 'ORGANIZATION_TYPE_Military',\n",
        " 'ORGANIZATION_TYPE_Mobile',\n",
        " 'ORGANIZATION_TYPE_Other',\n",
        " 'ORGANIZATION_TYPE_Police',\n",
        " 'ORGANIZATION_TYPE_Postal',\n",
        " 'ORGANIZATION_TYPE_Realtor',\n",
        " 'ORGANIZATION_TYPE_Religion',\n",
        " 'ORGANIZATION_TYPE_Restaurant',\n",
        " 'ORGANIZATION_TYPE_School',\n",
        " 'ORGANIZATION_TYPE_Security',\n",
        " 'ORGANIZATION_TYPE_Security Ministries',\n",
        " 'ORGANIZATION_TYPE_Self-employed',\n",
        " 'ORGANIZATION_TYPE_Services',\n",
        " 'ORGANIZATION_TYPE_Telecom',\n",
        " 'ORGANIZATION_TYPE_Trade: type 1',\n",
        " 'ORGANIZATION_TYPE_Trade: type 2',\n",
        " 'ORGANIZATION_TYPE_Trade: type 3',\n",
        " 'ORGANIZATION_TYPE_Trade: type 4',\n",
        " 'ORGANIZATION_TYPE_Trade: type 5',\n",
        " 'ORGANIZATION_TYPE_Trade: type 6',\n",
        " 'ORGANIZATION_TYPE_Trade: type 7',\n",
        " 'ORGANIZATION_TYPE_Transport: type 1',\n",
        " 'ORGANIZATION_TYPE_Transport: type 2',\n",
        " 'ORGANIZATION_TYPE_Transport: type 3',\n",
        " 'ORGANIZATION_TYPE_Transport: type 4',\n",
        " 'ORGANIZATION_TYPE_University',\n",
        " 'ORGANIZATION_TYPE_XNA',\n",
        " 'FLAG_DOCUMENT_3_0.0',\n",
        " 'FLAG_DOCUMENT_3_1.0',\n",
        " 'FLAG_DOCUMENT_21_0.0',\n",
        " 'FLAG_DOCUMENT_21_1.0',\n",
        " 'FLAG_WORK_PHONE_0.0',\n",
        " 'FLAG_WORK_PHONE_1.0',\n",
        " 'FLAG_OWN_CAR_0',\n",
        " 'FLAG_OWN_CAR_1',\n",
        " 'FLAG_CONT_MOBILE_0.0',\n",
        " 'FLAG_CONT_MOBILE_1.0',\n",
        " 'FLAG_DOCUMENT_8_0.0',\n",
        " 'FLAG_DOCUMENT_8_1.0',\n",
        " 'FLAG_EMP_PHONE_0.0',\n",
        " 'FLAG_EMP_PHONE_1.0',\n",
        " 'FLAG_DOCUMENT_4_0.0',\n",
        " 'FLAG_DOCUMENT_18_0.0',\n",
        " 'FLAG_DOCUMENT_18_1.0',\n",
        " 'NAME_TYPE_SUITE_Children',\n",
        " 'NAME_TYPE_SUITE_Family',\n",
        " 'NAME_TYPE_SUITE_Group of people',\n",
        " 'NAME_TYPE_SUITE_Other_A',\n",
        " 'NAME_TYPE_SUITE_Other_B',\n",
        " 'NAME_TYPE_SUITE_Spouse, partner',\n",
        " 'NAME_TYPE_SUITE_Unaccompanied',\n",
        " 'FLAG_DOCUMENT_15_0.0',\n",
        " 'FLAG_DOCUMENT_15_1.0',\n",
        " 'NAME_EDUCATION_TYPE_Academic degree',\n",
        " 'NAME_EDUCATION_TYPE_Higher education',\n",
        " 'NAME_EDUCATION_TYPE_Incomplete higher',\n",
        " 'NAME_EDUCATION_TYPE_Lower secondary',\n",
        " 'NAME_EDUCATION_TYPE_Secondary / secondary special',\n",
        " 'FLAG_DOCUMENT_6_0.0',\n",
        " 'FLAG_DOCUMENT_6_1.0',\n",
        " 'FLAG_OWN_REALTY_0',\n",
        " 'FLAG_OWN_REALTY_1',\n",
        " 'FLAG_DOCUMENT_5_0.0',\n",
        " 'FLAG_DOCUMENT_5_1.0',\n",
        " 'NAME_FAMILY_STATUS_Civil marriage',\n",
        " 'NAME_FAMILY_STATUS_Married',\n",
        " 'NAME_FAMILY_STATUS_Separated',\n",
        " 'NAME_FAMILY_STATUS_Single / not married',\n",
        " 'NAME_FAMILY_STATUS_Widow',\n",
        " 'FLAG_DOCUMENT_13_0.0',\n",
        " 'FLAG_DOCUMENT_13_1.0',\n",
        " 'FLAG_EMAIL_0.0',\n",
        " 'FLAG_EMAIL_1.0']\n",
        "\n",
        "dummy_cols = ['Unnamed: 0']\n",
        "target_col = 'TARGET'\n",
        "numeric_cols = list(set(train_data.columns) - set(category_cols + dummy_cols + [target_col]))\n",
        "\n",
        "def data_massage(data,  category_cols, numeric_cols):\n",
        "    feat_cols = category_cols + numeric_cols\n",
        "    fields = []\n",
        "    for feat_col in feat_cols:\n",
        "        if feat_col not in category_cols:\n",
        "            fields.append(1)\n",
        "        else:\n",
        "            fields.append(data[feat_col].nunique())\n",
        "    start_idx = [0] + np.cumsum(fields)[:-1].tolist()\n",
        "\n",
        "    return feat_cols, start_idx, fields\n",
        "\n",
        "class FMDataset(Dataset):\n",
        "    def __init__(self, data, feat_start_idx, fields_size, feat_cols, target_col):\n",
        "        self.data = data\n",
        "        self.label = np.asarray(self.data[target_col])\n",
        "\n",
        "        self.feat_cols = feat_cols\n",
        "        self.fields = fields_size\n",
        "        self.start_idx = feat_start_idx\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.loc[index, self.feat_cols]\n",
        "        idxs = list()\n",
        "        vals = list()\n",
        "        # label = self.data.loc[index, self.]\n",
        "        label = self.label[index]\n",
        "        for i in range(len(row)):\n",
        "            if self.fields[i] == 1:\n",
        "                idxs.append(self.start_idx[i])\n",
        "                vals.append(row[i])\n",
        "            else:\n",
        "                idxs.append(int(self.start_idx[i] + row[i]))\n",
        "                vals.append(1)\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        idxs = torch.tensor(idxs, dtype=torch.long)\n",
        "        vals = torch.tensor(vals, dtype=torch.float32)\n",
        "        \n",
        "        return label, idxs, vals\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "feat_cols, feat_start_idx, fields_size = data_massage(train_data,  category_cols, numeric_cols)\n",
        "\n",
        "args = {\n",
        "    'batch_size': 128,\n",
        "    'gpuid': '0',\n",
        "    'lr': 0.001,\n",
        "    'l2_reg': 0.,\n",
        "    'epochs': 30,\n",
        "    'num_features': len(feat_cols),\n",
        "    'embedding_dim': 8,\n",
        "    'field_size': fields_size,\n",
        "    'dense_size': [256, 512, 256, 128, 32],# [256, 512, 256, 128, 32]\n",
        "    '1o_dropout_p': 1, \n",
        "    '2o_dropout_p': 1, \n",
        "    'deep_dropout_p': 0.2,\n",
        "    'batch_norm': True,\n",
        "    'deep_layer_act': 'relu',\n",
        "    'opt_name': 'adam'\n",
        "}\n",
        "\n",
        "# train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "train_data, test_data = train_data.reset_index(drop=True), test_data.reset_index(drop=True)\n",
        "\n",
        "train_dataset = FMDataset(train_data, feat_start_idx, fields_size, feat_cols, target_col)\n",
        "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "test_dataset = FMDataset(test_data, feat_start_idx, fields_size, feat_cols, target_col)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
        "\n",
        "model = DeepFM(args)\n",
        "model.fit(train_loader)\n",
        "\n",
        "prediction = model.predict(test_loader)\n",
        "print('AUC score: ', roc_auc_score(test_data[target_col].values, prediction))\n",
        "prediction_label = np.where(prediction.detach().numpy() >= 0.5, 1, 0)\n",
        "print('Recall score: ', recall_score(test_data[target_col].values, prediction_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRcSW8KEiITr",
        "outputId": "b98c2fce-06de-40f1-a93f-0343a1b06c62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 001] - training loss=0.8532\n",
            "[Epoch 002] - training loss=0.7397\n",
            "[Epoch 003] - training loss=0.7021\n",
            "[Epoch 004] - training loss=0.6904\n",
            "[Epoch 005] - training loss=0.6849\n",
            "[Epoch 006] - training loss=0.6817\n",
            "[Epoch 007] - training loss=0.6802\n",
            "[Epoch 008] - training loss=0.6783\n",
            "[Epoch 009] - training loss=0.6775\n",
            "[Epoch 010] - training loss=0.6773\n",
            "[Epoch 011] - training loss=0.6766\n",
            "[Epoch 012] - training loss=0.6762\n",
            "[Epoch 013] - training loss=0.6761\n",
            "[Epoch 014] - training loss=0.6767\n",
            "[Epoch 015] - training loss=0.6756\n",
            "[Epoch 016] - training loss=0.6755\n",
            "[Epoch 017] - training loss=0.6748\n",
            "[Epoch 018] - training loss=0.6750\n",
            "[Epoch 019] - training loss=0.6750\n",
            "[Epoch 020] - training loss=0.6746\n",
            "[Epoch 021] - training loss=0.6745\n",
            "[Epoch 022] - training loss=0.6747\n",
            "[Epoch 023] - training loss=0.6755\n",
            "[Epoch 024] - training loss=0.6743\n",
            "[Epoch 025] - training loss=0.6743\n",
            "[Epoch 026] - training loss=0.6748\n",
            "[Epoch 027] - training loss=0.6747\n",
            "[Epoch 028] - training loss=0.6743\n",
            "[Epoch 029] - training loss=0.6739\n",
            "[Epoch 030] - training loss=0.6744\n",
            "AUC score:  0.84923935178055\n",
            "Recall score:  0.8325581395348837\n"
          ]
        }
      ]
    }
  ]
}
